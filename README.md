# InforceTask

## **Description**
This project demonstrates an ETL (Extract, Transform, Load) pipeline:
- **Extract**: Synthetic user data is generated using the Faker library.
- **Transform**: Data is cleaned and enriched (e.g., email domains are extracted).
- **Load**: Transformed data is loaded into a PostgreSQL database.

The project is containerized using Docker and includes a PostgreSQL database for data storage. The ETL process is implemented in Python.

---

## **Table of Contents**
1. [Features](#features)
2. [Technologies Used](#technologies-used)
3. [Installation](#installation)
4. [Usage](#usage)
5. [Database Schema](#database-schema)
6. [SQL Queries](#sql-queries)
7. [Assumptions](#assumptions)
8. [Contributing](#contributing)

---

## **Features**
- Automatically generates synthetic user data.
- Validates and cleans the data.
- Loads data into a PostgreSQL database.
- Fully containerized setup using Docker and Docker Compose.

---

## **Technologies Used**
- **Python**: Core language for ETL pipeline.
- **Faker**: Library for generating fake data.
- **Pandas**: Data manipulation and cleaning.
- **SQLAlchemy**: Database connection and management.
- **PostgreSQL**: Database backend.
- **Docker & Docker Compose**: Containerized deployment.

---

## **Installation**

### **Prerequisites**
- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)

### **Steps**
1. **Clone the repository**:
   ```bash
   git clone https://github.com/skezzy1/InforceTask.git
   cd <InforceTask>
2. **Create a .env file in the root directory with the following content**:
    ```bash
    POSTGRES_DB = etl_db
    POSTGRES_PASSWORD = 1111
    POSTGRES_USER = root
    POSTGRES_PORT = 5432
    POSTGRES_PORTS = 5432:5432
    db_url = postgresql://root:1111@db:5432/etl_db
    SQL_FILE_PATH = /app/create_table.sql
    csv_name = /app/users.csv
3. **Build the Docker containers**:
    ```bash 
    docker-compose build
4. **Start the containers**:
    ```bash 
    docker-compose up
5. **Verify the containers are running**:
   After running the containers, you can verify that everything is working by checking the logs from Docker Compose:
   ```bash
   docker-compose logs
### **Usage**
Once the containers are up and running, the ETL pipeline will automatically execute. Here's how the process flows:

1. **Data Generation**:
* Synthetic user data is generated using the Faker library.
* This data includes user IDs, names, emails, and signup dates.
2. **Data Transformation**:
* The data is cleaned by validating email formats.
* The domain is extracted from each email.
3. **Data Loading**:
* The transformed data is loaded into a PostgreSQL database table called users.
4. **Check CSV Output**:
* The generated CSV file will be saved at **app/users.csv**. You can open this CSV file to inspect the raw data before it is loaded into the database.
---
--- 

## **Database Schema**
- The project uses a PostgreSQL database with the following schema to store the user data:
![Database Schema](https://imgur.com/a/Qp3Pcue)
#### Table Explanation:

* **user_id**: A unique identifier for each user.
* **name**: The user's full name.
* **email**: The user's email address.
* **signup_date**: The date and time when the user signed up.
* **domain**: The domain part of the user's email (extracted from the email address).

## **SQL Queries**
- Here are some example SQL queries you can use to interact with the users table:
1. Retrieve all users:
    ```bash 
    SELECT * FROM users;
2. Find users by domain:
    ```bash 
    SELECT * FROM users WHERE domain = 'gmail.com';
3. Delete all users:
    ```bash 
    DELETE FROM users;

## **Assumptions**
1. Data Format:

**Assumption**: The synthetic data generated by the Faker library will have a consistent format (i.e., valid user names, emails, and dates).
**Verification**: You can verify this by inspecting the generated users.csv file. Each row should contain a valid name, email, and signup date. You can also check for the validity of emails by applying the same regex pattern used in the transform_data function.

2. Email Validation:
**Assumption**: Only valid emails are included in the data.
**Verification**: After the transform_data function runs, you can inspect the transformed DataFrame (transformed_data) to ensure that all emails match the specified pattern. Additionally, you can query the PostgreSQL database to verify that all email entries are valid using SQL queries.

3. Containerized Environment:
**Assumption**: The project runs in a Docker containerized environment using Docker Compose for multi-container management (PostgreSQL + Python application).
**Verification**: Once the containers are running **(docker-compose up)**, you can verify that the application and database are running by checking the logs **(docker-compose logs)**. You should see logs indicating that the ETL process has started and completed.

4. Database Setup:
Assumption: The PostgreSQL database will be correctly set up using the create_table.sql file, which creates a users table.
Verification: You can verify the table creation by connecting to the PostgreSQL database and running the query **SELECT * FROM users;**. The users table should exist with the correct schema and contain data.

5. CSV File Location:
Assumption: The users.csv file is generated and saved in the correct location **(/app/users.csv)**.
Verification: After running the ETL process, check the app folder of your project to ensure the users.csv file is present. You can open the CSV file to verify that it contains the expected data.
Data Transformations:

6. Assumption: The email domain is extracted correctly, and the signup date is formatted properly.
Verification: You can inspect the transformed data in the transformed_data DataFrame or directly query the PostgreSQL database using SQL to verify that the domain column is populated correctly, and the **signup_date** is in the correct format.

7. ETL Process Executed Successfully:
Assumption: The entire ETL pipeline (Generate → Transform → Load) will complete without errors.
Verification: Check the logs from the Docker containers (docker-compose logs). You should see messages indicating that the CSV file was generated, data was transformed, and the data was successfully loaded into the PostgreSQL database.

8. Database Connectivity:
Assumption: The Python application will be able to connect to the PostgreSQL database using the **db_url** defined in the .env file.
Verification: If the ETL process completes successfully without connection errors, the assumption holds. You can also manually test the database connection by using a PostgreSQL client like psql or a GUI tool (e.g., DBeaver) and connecting to the database using the credentials defined in the **.env** file.